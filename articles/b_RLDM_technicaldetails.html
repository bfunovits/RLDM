<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="RLDM">
<title>Rational Linear Dynamic Models (RLDM): Technical Details • RLDM</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Rational Linear Dynamic Models (RLDM): Technical Details">
<meta property="og:description" content="RLDM">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">RLDM</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9006</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/a_RLDM.html">Rational Linear Dynamic Models (RLDM)</a>
    <a class="dropdown-item" href="../articles/b_RLDM_technicaldetails.html">Rational Linear Dynamic Models (RLDM): Technical Details</a>
    <a class="dropdown-item" href="../articles/c_hrk_stage3.html">Hannan-Rissanen-Kavalieris Estimation - Stage 3</a>
    <a class="dropdown-item" href="../articles/d_casestudy2.html">Case Study</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/bfunovits/RLDM/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Rational Linear Dynamic Models (RLDM): Technical Details</h1>
                        <h4 data-toc-skip class="author">Wolfgang
Scherrer and Bernd Funovits</h4>
            
            <h4 data-toc-skip class="date">2023-12-17</h4>
      
      
      <div class="d-none name"><code>b_RLDM_technicaldetails.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="estimate-ar-models">Estimate AR Models<a class="anchor" aria-label="anchor" href="#estimate-ar-models"></a>
</h2>
<p>Let <span class="math inline">\((y_t)\)</span> be the <span class="math inline">\(m\)</span>-dimensional AR(<span class="math inline">\(p\)</span>) process defined by the <em>stable</em>
AR(<span class="math inline">\(p\)</span>) system</p>
<p><span class="math display">\[
y_t = a_1 y_{t-1} + \cdots + a_p y_{t-p} + u_t
\]</span> <span class="math inline">\((u_t)\)</span> is a white noise
process with covariance matrix <span class="math inline">\(\Sigma=\mathbb{E} u_t u_t' \in \mathbb{R}^{m
\times m}\)</span>.</p>
<div class="section level3">
<h3 id="yule-walker-estimate">Yule-Walker Estimate<a class="anchor" aria-label="anchor" href="#yule-walker-estimate"></a>
</h3>
<p>If the above AR(<span class="math inline">\(p\)</span>) model is
stable (i.e. <span class="math inline">\(\det(I_m - a_1 z - \cdots - a_p
z^p)\)</span> is non zero for all <span class="math inline">\(z\in
\mathbb{C}\)</span> with <span class="math inline">\(|z|\leq 1\)</span>)
then the autocovariance function <span class="math inline">\((\gamma_k =
\mathbb{E} y_{t+k} y_t')\)</span> and the parameters of the model
satisfy the socalled <em>Yule-Walker</em> equations:</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_0 &amp;= a_1  \gamma_{-1} + \cdots + a_p \gamma_{-p} + \Sigma \\
\gamma_k &amp;= a_1  \gamma_{k-1} + \cdots + a_p \gamma_{k-p} &amp;
\mbox{ for } k &gt; 0
\end{aligned}
\]</span></p>
<p>If we use the notation <span class="math display">\[
a = (a_p,\ldots,a_1), \;
y_{t-1}^{p} = (y_{t-p}',\ldots,y_{t-1}'), \;
\Gamma_p = \mathbb{E} y_{t-1}^{p} (y_{t-1}^{p})' \mbox{ and } \;
\gamma_1^p = \mathbb{E} y_t (y_{t-1}^p)' =
(\gamma_p,\ldots,\gamma_1)
\]</span> then the Yule-Walker equations (for <span class="math inline">\(k=0,\ldots,p\)</span>) may be written as <span class="math display">\[
\begin{aligned}
\gamma_0 &amp;= a (\gamma_1^p)' + \Sigma \\
\gamma_1^p &amp;= a \Gamma_p
\end{aligned}
\]</span></p>
<p>To solve these equations we use the Cholesky decomposition of the
Toeplitz matrix</p>
<p><span class="math display">\[
\Gamma_{p+1} = \mathbb{E} \begin{pmatrix} y_{t-1}^p \\ y_t \end{pmatrix}
                  \begin{pmatrix} y_{t-1}^p \\ y_t \end{pmatrix}'
       = \begin{pmatrix}
         \Gamma_p &amp; (\gamma_1^p)' \\
         \gamma_1^p &amp; \gamma_{0}
         \end{pmatrix}
       = \begin{pmatrix}
         R_{11}' &amp; 0_{mp\times m} \\
         R_{12}' &amp; R_{22}'
         \end{pmatrix}
         \begin{pmatrix}
         R_{11}  &amp; R_{12} \\
         0_{m\times mp}       &amp; R_{22}
         \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
a &amp;=  \gamma_1^p \Gamma_p^{-1}  = R_{12}' R_{11}
(R_{11}'R_{11})^{-1} = R_{12}' R_{11}^{-T} \\
\Sigma &amp;= \gamma_0 - a (\gamma_1^p)' =  R_{12}' R_{12}
+  R_{22}' R_{22} - R_{12}' R_{11}^{-T} R_{11}' R_{12} =
R_{22}'R_{22}
\end{aligned}
\]</span></p>
<p>One advantage of this approach is that we can easily “read off” the
noise covariances for all AR models with orders <span class="math inline">\(0,1,\ldots,p\)</span> from the cholesky factor
<span class="math inline">\(R\)</span>. Therefore the determinant of
these noise covariances may be computed from the diagonal elements of
<span class="math inline">\(R\)</span>. E.g. for the model with order
<span class="math inline">\(p\)</span> we have <span class="math display">\[
\log\det\Sigma_p = 2 \sum_{i=mp+1}^{m(p+1)} \log r_{ii}
\]</span> where <span class="math inline">\(r_{ii}\)</span> denotes the
<span class="math inline">\(i\)</span>-th diagonal element of <span class="math inline">\(R\)</span>.</p>
<p>This algorithm is implemented in the function</p>
<pre><code><span><span class="fu"><a href="../reference/est_ar.html">est_ar_yw</a></span><span class="op">(</span><span class="va">gamma</span>, p.max <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, penalty <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="durbin-levinson-whittle-algorithm">Durbin-Levinson-Whittle Algorithm<a class="anchor" aria-label="anchor" href="#durbin-levinson-whittle-algorithm"></a>
</h3>
<p>Let <span class="math inline">\((y_{t} \,|\, t \in
\mathbb{Z})\)</span> denote a discrete time, stationary process with
mean zero and autocovariance function <span class="math inline">\(\gamma_k = \mathbb{E} y_{t+k} y_t'\)</span>.
The Durbin-Levinson-Whittle procedure computes the linear, least
squares, one-step ahead predictions <em>recursively</em> in the number
of past values used for the prediction. See <span class="citation">(Whittle 1963)</span>.</p>
<p>The one-step ahead prediction for <span class="math inline">\(y_t\)</span> given <span class="math inline">\(p\)</span> past values <span class="math display">\[
y_{t|p} = a^{(p)}_1 y_{t-1} + \cdots + a^{(p)}_p y_{t-p}
\]</span> is the orthogonal projection of <span class="math inline">\(y_t\)</span> onto the space</p>
<p><span class="math display">\[
\mathbb{H}_{t-p}^{t-1} = \mathrm{span}\{y_{t-1}, \ldots , y_{t-p}\}.
\]</span> If we denote the corresponding projection operator with <span class="math inline">\(\mathbb{P}_{t-p}^{t-1}\)</span> then we can write
<span class="math inline">\(y_{t|p}= \mathbb{P}_{t-p}^{t-1} \,
y_t\)</span>. The forecast error and its covariance matrix are denoted
with</p>
<p><span class="math display">\[
\begin{aligned}
u_{t|p} &amp; = y_t - y_{t|p} = y_t - a^{(p)}_1 y_{t-1} - \cdots -
a^{(p)}_p y_{t-p} \\
\Sigma_p &amp; = \mathbb{E} u_{t|p} u_{t|p}' = \gamma_0 - \mathbb{E}
y_{t|p} y_{t|p}'
\end{aligned}
\]</span></p>
<p>The main trick, to derive the Durbin-Levinson-Whittle recursions, is
to consider also <em>backcasts</em>, i.e. the linear, least squares,
approximations of <span class="math inline">\(y_s\)</span> in terms of
future values of the process. We define</p>
<p><span class="math display">\[
\begin{aligned}
y^{b}_{s|p} &amp; = \mathbb{P}_{s+1}^{s+p} y_s = b^{(p)}_1 y_{s+1} +
\cdots + b^{(p)}_p y_{s+p} \\
v_{s|p} &amp; = y_s - y^{b}_{s|p} = y_s - b^{(p)}_1 y_{s+1} - \cdots -
b^{(p)}_p y_{s+p} \\
\Omega_p &amp; = \mathbb{E} v_{s|p} v_{s|p}' = \gamma_0 - \mathbb{E}
y^{b}_{s|p} (y^{b}_{s|p})'
\end{aligned}
\]</span></p>
<p>The Durbin-Levinson-Whittle recursions consist of the following
equations:</p>
<p><span class="math display">\[
\begin{aligned}
\Delta_{p}' = \mathbb{E} v_{t-p|t-1} y_t ' = \mathbb{E} y_{t-p}
u_{t|t-1}' &amp; =
    \gamma_{-p} - b_{1}^{(p-1)} \gamma_{1-p} - \cdots - b_{p-1}^{(p-1)}
\gamma_{-1} \\
    a_p^{(p)} &amp;= \Delta_p \Omega_{p-1}^{-1} \\
\left( a_{1}^{(p)}, \ldots, a_{p-1}^{(p)} \right) &amp;=   
    \left(a_{1}^{(p-1)}, \ldots, a_{p-1}^{(p-1)} \right) -
    a_p^{(p)} \left(b_{p-1}^{(p-1)}, \ldots, b_{1}^{(p-1)} \right) \\
    b_p^{(p)} &amp;= \Delta_p' \Sigma_{p-1}^{-1} \\
\left( b_{1}^{(p)}, \ldots, b_{p-1}^{(p)} \right) &amp;=
   \left( b_{1}^{(p-1)}, \ldots, b_{p-1}^{(p-1)} \right) -
   b_p^{(p)} \left(a_{p-1}^{(p-1)}, \ldots, a_{1}^{(p-1)} \right) \\
\Sigma_{p} &amp; = \Sigma_{p-1} - \Delta_p \Omega_{p-1}^{-1}
\Delta_p' =
               \Sigma_{p-1} - a^{(p)}_p \Delta_p' =
               \left( I_{n} - a_{p}^{(p)} b_{p}^{(p)} \right)
\Sigma_{p-1}\\
\Omega_{p} &amp; = \Omega_{p-1} - \Delta_p' \Sigma_{p-1}^{-1}
\Delta_p =
               \Omega_{p-1} - b^{(p)}_p \Delta_p =
               \left( I_{n} - b_{p}^{(p)} a_{p}^{(p)} \right)
\Omega_{p-1}
\end{aligned}
\]</span></p>
<p>The recursions start with <span class="math inline">\(p=1\)</span>
and <em>initial values</em> <span class="math inline">\(\Sigma_0 =
\Omega_0 = \gamma_0\)</span>.</p>
<div class="section level4">
<h4 id="proof">Proof<a class="anchor" aria-label="anchor" href="#proof"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="express-the-forecast-y_tp-as-the-sum-of-two-projections-onto-orthogonal-spaces">Express the forecast <span class="math inline">\(y_{t|p}\)</span> as the sum of two projections
onto orthogonal spaces<a class="anchor" aria-label="anchor" href="#express-the-forecast-y_tp-as-the-sum-of-two-projections-onto-orthogonal-spaces"></a>
</h5>
<p>The prediction <span class="math inline">\(y_{t|p}\)</span> is the
projection of <span class="math inline">\(y_{t}\)</span> onto the space
<span class="math inline">\(\mathbb{H}_{t-p}^{t-1} =
\mathrm{span}\{y_{t-p},\ldots, y_{t-1}\}\)</span>. This space can be
decomposed into the sum of the two orthogonal subspaces</p>
<ul>
<li>
<span class="math inline">\(\mathbb{H}_{t-p+1}^{t-1} =
\mathrm{span}\{y_{t-p+1},\ldots, y_{t-1}\}\)</span> and</li>
<li>the span of the components of the backcast error <span class="math inline">\(v_{t-p|p-1} = y_{t-p} - \mathbb{P}_{t-p+1}^{t-1}
y_{t-p} =  y_{t-p} - b^{(p-1)}_1 y_{t-p+1} - \cdots - b^{(p-1)}_{p-1}
y_{t-1}.\)</span>
</li>
</ul>
<p>Therefore <span class="math inline">\(y_{t|p}\)</span> is equal to
the sum of the projection of <span class="math inline">\(y_t\)</span>
onto <span class="math inline">\(\mathbb{H}_{t-p+1}^{t-1}\)</span>
(i.e. <span class="math inline">\(y_{t|p-1}\)</span>) and the projection
of <span class="math inline">\(y_t\)</span> onto the space spanned by
<span class="math inline">\(v_{t-p|p-1}\)</span>. The second projection,
which represents the contribution of <span class="math inline">\(y_{t-p}\)</span> to the prediction, can be
expressed as <span class="math display">\[
\begin{aligned}
(\mathbb{E} y_{t} v_{t-p|p-1}')(\mathbb{E} v_{t-p|p-1}
v_{t-p|p-1}')^{-1} v_{t-p|p-1} = \\
(\mathbb{E} (y_{t-p} - b^{(p-1)}_1 y_{t-p+1} - \cdots - b^{(p-1)}_{p-1}
y_{t-1}) y_t' )'
  \Omega_{p-1}^{-1} (y_{t-p} - b^{(p-1)}_1 y_{t-p+1} - \cdots -
b^{(p-1)}_{p-1} y_{t-1}) = \\
(\gamma_{-p} - b^{(p-1)}_1 \gamma_{1-p} - \cdots - b^{(p-1)}_{p-1}
\gamma_{-1})'
  \Omega_{p-1}^{-1} (y_{t-p} - b^{(p-1)}_1 y_{t-p+1} - \cdots -
b^{(p-1)}_{p-1} y_{t-1})
\end{aligned}
\]</span> Collecting the two projections, we obtain</p>
<p><span class="math display">\[
\begin{aligned}
y_{t|p} = \mathbb{P}_{t-p}^{t-1} y_t  
&amp;=
a_{1}^{(p-1)}y_{t-1}+\cdots+a_{p-1}^{(p-1)}y_{t-p+1} +
\Delta_p \Omega_{p-1}^{-1} (y_{t-p} - b^{(p-1)}_1 y_{t-p+1} - \cdots -
b^{(p-1)}_{p-1} y_{t-1})\\
&amp;=
\left(\left(a_{1}^{(p-1)},\ldots,a_{p-1}^{(p-1)},0\right) +
      \Delta_p \Omega_{p-1}^{-1}
\left(-b_{p-1}^{(p-1)},\ldots,-b^{(p-1)}_1,I_{n}\right)\right)
\begin{pmatrix}y_{t-1}\\
\vdots\\
y_{t-p+1}\\
y_{t-p}
\end{pmatrix}
\end{aligned}
\]</span> where <span class="math display">\[
\Delta_p = (\mathbb{E} v_{t-p|p-1} y_t')' =
(\gamma_{-p} - b^{(p-1)}_1 \gamma_{1-p} - \cdots - b^{(p-1)}_{p-1}
\gamma_{-1})'.
\]</span></p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="express-the-backcast-yb_t-pp-as-the-sum-of-two-projections-onto-orthogonal-spaces">Express the backcast <span class="math inline">\(y^b_{t-p|p}\)</span> as the sum of two projections
onto orthogonal spaces<a class="anchor" aria-label="anchor" href="#express-the-backcast-yb_t-pp-as-the-sum-of-two-projections-onto-orthogonal-spaces"></a>
</h5>
<p>We first decompose the space <span class="math inline">\(\mathbb{H}_{t-p+1}^{t}\)</span> into the
orthogonal sum of the spaces <span class="math inline">\(\mathbb{H}_{t-p+1}^{t-1} =
\mathrm{span}\{y_{t-p+1},\ldots,y_{t-1}\}\)</span> and the span of the
components of forecast error <span class="math inline">\(u_{t|p-1} = y_t
- a^{(p-1)}_1 y_{t-1} - \cdots - a^{(p-1)}_{p-1} y_{t-p+1}\)</span>.</p>
<p>The projection of <span class="math inline">\(y_{t-p}\)</span> onto
the space spanned by the forecast error <span class="math inline">\(u_{t|p-1}\)</span> is given by <span class="math display">\[
\begin{aligned}
(\mathbb{E} y_{t-p} u_{t|p-1}')(\mathbb{E} u_{t|p-1}
u_{t|p-1}')^{-1} u_{t|p-1} = \\
(\mathbb{E} (y_t - a^{(p-1)}_1 y_{t-1} - \cdots - a^{(p-1)}_{p-1}
y_{t-p+1}) y_{t-p}')'
  \Sigma_{p-1}^{-1} (y_t - a^{(p-1)}_1 y_{t-1} - \cdots -
a^{(p-1)}_{p-1} y_{t-p+1}) = \\
(\gamma_{p} - a^{(p-1)}_1 \gamma_{p-1} - \cdots - a^{(p-1)}_{p-1}
\gamma_{1})'
  \Sigma_{p-1}^{-1} (y_t - a^{(p-1)}_1 y_{t-1} - \cdots -
a^{(p-1)}_{p-1} y_{t-p+1})
\end{aligned}
\]</span> Therefore the backcast <span class="math inline">\(y^{b}_{t-p|p}\)</span> is equal to <span class="math display">\[
\begin{aligned}
y^{b}_{t-p|p} = \mathbb{P}_{t-p+1}^{t} y_{t-p}   
&amp;=
b_{1}^{(p-1)}y_{t-p+1}+\cdots+b_{p-1}^{(p-1)}y_{t-1} +
\Delta^b_{p} \Sigma_{p-1}^{-1} (y_{t} - a^{(p-1)}_1 y_{t-1} - \cdots -
b^{(p-1)}_{p-1} y_{t-p+1})\\
&amp;=
\left(\left(b_{1}^{(p-1)},\ldots,b_{p-1}^{(p-1)},0\right) +
      \Delta^b_{p} \Sigma_{p-1}^{-1}
\left(-a_{p-1}^{(p-1)},\ldots,-a^{(p-1)}_1,I_{n}\right)\right)
\begin{pmatrix}y_{t-p+1}\\
\vdots\\
y_{t-1}\\
y_{t}
\end{pmatrix}
\end{aligned}
\]</span> where <span class="math display">\[
\Delta^b_p = (\mathbb{E} u_{t|p-1} y_{t-p}')' =
\left( \gamma_{p} - a^{(p-1)}_1 \gamma_{p-1} - \cdots - a^{(p-1)}_{p-1}
\gamma_{1} \right)'.
\]</span></p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="covariances-of-the-forecast-error-and-backcast-errors">Covariances of the forecast error and backcast
errors<a class="anchor" aria-label="anchor" href="#covariances-of-the-forecast-error-and-backcast-errors"></a>
</h5>
<p>The covariance matrix of the forecast error <span class="math inline">\(u_{t|p}\)</span> is (due to the orthogonality)
<span class="math display">\[
\Sigma_p = \mathbb{E} (u_{t|p} u_{t|p}') =  \mathbb{E} (u_{t|p-1}
u_{t|p-1}') -
\mathbb{E} ((\Delta_p \Omega_{p-1}^{-1} v_{t-p|p-1}) (\Delta_p
\Omega_{p-1}^{-1} v_{t-p|p-1})') =
\Sigma_{p-1} - \Delta_p \Omega_{p-1}^{-1} \Delta_p'
\]</span> and analogously the covariance matrix of the backcast error is
given by <span class="math display">\[
\Omega_p = \mathbb{E} (\bar u_{t-p|p} \bar u_{t-p|p}') =  \mathbb{E}
(\bar u_{t-p|p-1} \bar u_{t-p|p-1}') -
\mathbb{E} ((\Delta^b_p \Sigma_{p-1}^{-1} u_{t|p-1}) (\Delta^b_p
\Sigma_{p-1}^{-1} u_{t|p-1})') =
\Omega_{p-1} - \Delta^b_p \Sigma_{p-1}^{-1} (\Delta^b_p)'.
\]</span></p>
<p>Note that <span class="math display">\[
\Delta_p = \mathbb{E} y_t v_{t-p|p-1}' = \mathbb{E} u_{t|p-1}
v_{t-p|p-1}'
         = \mathbb{E} u_{t|p-1} y_{t-p}' = (\Delta^b_p)'.
\]</span></p>
<p>Together with <span class="math inline">\(a^{(p)}_p = \Delta_p
\Omega_{p-1}^{-1}\)</span> and <span class="math inline">\(b^{(p)}_p =
\Delta_p' \Sigma_{p-1}^{-1}\)</span> we thus obtain the desired
recursions for the covariance matrices <span class="math display">\[
\begin{aligned}
\Sigma_p &amp; = \Sigma_{p-1} - \Delta_p \Omega_{p-1}^{-1} \Delta_p'
=
             \Sigma_{p-1} -  a_p^{(p)} \Delta_p' =
             \Sigma_{p-1} -  a_p^{(p)} b_p^{(p)}\Sigma_{p-1} \\
\Omega_p &amp; = \Omega_{p-1} - \Delta^b_p \Sigma_{p-1}^{-1}
(\Delta^b_p)' =
       \Omega_{p-1} -  b_p^{(p)} \Delta_p =
       \Omega_{p-1} -  b_p^{(p)} a_p^{(p)}\Omega_{p-1}
\end{aligned}
\]</span></p>
</div>
</div>
<div class="section level4">
<h4 id="implementation">Implementation<a class="anchor" aria-label="anchor" href="#implementation"></a>
</h4>
<p>The above described Durbin-Levinson-Wittle recursion is implemented
in the function</p>
<pre><code><span><span class="fu"><a href="../reference/est_ar.html">est_ar_dlw</a></span><span class="op">(</span><span class="va">gamma</span>, p.max <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, penalty <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span></code></pre>
<p>This procedure in addition computes the partial autocorrelation
function, i.e.  <span class="math display">\[
\begin{aligned}
\delta_p &amp;= (\mbox{diag}(\mathbb{E} u_{t|p-1}u_{t|p-1}'))^{-1/2}
            ( \mathbb{E} u_{t|p-1} v_{t-p|p-1}' )
            (\mbox{diag}(\mathbb{E} v_{t-p|p-1}v_{t-p|p-1}'))^{-1/2}
\\
   &amp;= (\mbox{diag}(\Sigma_{p-1}))^{-1/2} ( \Delta_p )
(\mbox{diag}(\Omega_{p-1}))^{-1/2}  &amp; \mbox{ for } p \geq 0
\end{aligned}
\]</span></p>
</div>
</div>
<div class="section level3">
<h3 id="ols-estimate">OLS Estimate<a class="anchor" aria-label="anchor" href="#ols-estimate"></a>
</h3>
<p>Let</p>
<p><span class="math display">\[
Y = \begin{pmatrix}
    y_1' &amp; \cdots &amp; y_{p}' &amp; y_{p+1}' \\
    y_2' &amp; \cdots &amp; y_{p+1}' &amp; y_{p+2}' \\
    \vdots &amp;     &amp; \vdots &amp; \vdots \\
    y_{N-p}' &amp; \cdots &amp; y_{N-1}' &amp; y_{N}' \\
    \end{pmatrix}
  = (Q_1 \; Q_2) \begin{pmatrix} R_{11} &amp; R_{12} \\ 0 &amp; R_{22}
\end{pmatrix}
\]</span></p>
</div>
<div class="section level3">
<h3 id="note-on-numerical-issues---rd">note on numerical issues -&gt; Rd<a class="anchor" aria-label="anchor" href="#note-on-numerical-issues---rd"></a>
</h3>
</div>
<div class="section level3">
<h3 id="wrapper-function">wrapper function<a class="anchor" aria-label="anchor" href="#wrapper-function"></a>
</h3>
<p>store mean, intercept!</p>
</div>
</div>
<div class="section level2">
<h2 id="arma-processes">ARMA Processes<a class="anchor" aria-label="anchor" href="#arma-processes"></a>
</h2>
<div class="section level3">
<h3 id="computation-of-the-acf">Computation of the ACF<a class="anchor" aria-label="anchor" href="#computation-of-the-acf"></a>
</h3>
<p><span class="math display">\[
a_0 y_t + a_1 y_{t-1} \cdots + a_p y_{t-p} = b_0 \epsilon_t + b_1
\epsilon_{t-1} + \cdots b_q \epsilon_{t-q}
\]</span></p>
<p>We assume that the stability condition holds and thus the stationary
solution of the ARMA system is the causal MA(<span class="math inline">\(\infty\)</span>) process <span class="math display">\[
y_t = \sum_{i \geq 0} k_i \epsilon_{t-i}.
\]</span></p>
<p>This implies in particular</p>
<p><span class="math display">\[
\mathbb{E} \epsilon_{t-i}y_t' =
\begin{cases}
     \Sigma k_i' &amp; \mbox{ for } i \geq 0 \\
     0           &amp; \mbox{ for } i &lt; 0.
\end{cases}  
\]</span></p>
<p>The genaralized Yule-Walker equations now are obtained by multiplying
the ARMA system from the right with <span class="math inline">\(y_{t-j}'\)</span> and taking expectations
<span class="math display">\[
\begin{aligned}
a_0 \gamma_j + a_1 \gamma_{j-1} + \cdots + a_p \gamma_{j-p}
&amp; = b_0 \mathbb{E} \epsilon_t y_{t-j}' + b_1 \mathbb{E}
\epsilon_{t-1} y_{t-j}' + \cdots + b_q \mathbb{E} \epsilon_{t-q}
y_{t-j}'  \\
&amp; = b_j \Sigma k_0' + \cdots + b_q \Sigma k_{q-j}'  &amp;
\mbox{for } 0 \leq j \leq q \\
&amp; = 0 &amp; \mbox{ for } j&gt; q
\end{aligned}
\]</span> In the following the right hand sides of these equations will
be denoted with <span class="math inline">\(\Delta_j\)</span>, <span class="math inline">\(j\geq 0\)</span>.</p>
<p>In a first step we consider the equations for <span class="math inline">\(j=0,\ldots,p\)</span> and solve these equations
for <span class="math inline">\(\gamma_0, \ldots, \gamma_p\)</span>. To
this end we consider the “vectorised” covariances <span class="math inline">\(\mathrm{vec}(\gamma_k)\)</span> and note that
<span class="math display">\[
\mathrm{vec}(a_i \gamma_k) = (I_m \otimes a_i) \mathrm{vec} (\gamma_k)
\mbox{ and }
\mathrm{vec}(a_i \gamma_{-k}) = \mathrm{vec}(a_i \gamma_{k}') = (I_m
\otimes a_i) \mathrm{vec}(\gamma_k') =
(I_m \otimes a_i) P \mathrm{vec} (\gamma_k)
\]</span> where <span class="math inline">\(P \in\mathbb{R}^{m^2 \times
m^2}\)</span> is a permutation matrix.</p>
<p>As a simple example consider the case <span class="math inline">\(p=2\)</span>: The Yule-Walker equations for <span class="math inline">\(j=0,\ldots,2\)</span> <span class="math display">\[
\begin{array}{rrrcr}
a_0 \gamma_0   &amp;+ a_1 \gamma_{-1} &amp;+ a_2 \gamma_{-2} &amp;=&amp;
\Delta_0 \\
a_0 \gamma_{1} &amp;+ a_1 \gamma_{0}  &amp;+ a_2 \gamma_{-1} &amp;=&amp;
\Delta_1 \\
a_0 \gamma_{2} &amp;+ a_1 \gamma_{1}  &amp;+ a_2 \gamma_{0}  &amp;=&amp;
\Delta_2 \\
\end{array}
\]</span> give the following “vectorized” equation system:</p>
<p><span class="math display">\[
\begin{array}{rrrcr}
(I_m \otimes a_0) \mathrm{vec}(\gamma_0)   &amp;+ (I_m \otimes a_1)P
\mathrm{vec}(\gamma_{1})
                                           &amp;+ (I_m \otimes a_2)P
\mathrm{vec}(\gamma_{2}) &amp;=&amp; \mathrm{vec}(\Delta_0) \\
(I_m \otimes a_1) \mathrm{vec}(\gamma_{0}) &amp;+ ((I_m \otimes
a_0)+(I_m \otimes a_2)P) \mathrm{vec}(\gamma_{1})
                                           &amp;+ 0
\mathrm{vec}(\gamma_{1}) &amp;=&amp; \mathrm{vec}(\Delta_1) \\
(I_m \otimes a_2) \mathrm{vec}(\gamma_{0}) &amp;+ (I_m \otimes a_1)
\mathrm{vec}(\gamma_{1})  
                                           &amp;+ (I_m \otimes a_0)
\mathrm{vec}(\gamma_{2}) &amp;=&amp; \mathrm{vec}(\Delta_2)
\end{array}
\]</span> In the second step the AR coefficients <span class="math inline">\(\gamma_j\)</span>, <span class="math inline">\(j&gt;p\)</span> are determined by the recursion
<span class="math display">\[
\gamma_j = \Delta_j - a_0^{-1}a_1 \gamma_{j-1} - \cdots - a_0^{-1}a_p
\gamma_{j-p} \mbox{ for } j &gt; p
\]</span></p>
<p>This procedure is implemented in</p>
<pre><code><span><span class="fu"><a href="../reference/autocov.html">autocov.armamod</a></span><span class="op">(</span><span class="va">obj</span>, type<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">'covariance'</span>,<span class="st">'correlation'</span>,<span class="st">'partial'</span><span class="op">)</span>, lag.max <span class="op">=</span> <span class="fl">12</span><span class="op">)</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="solve-difference-equations">Solve Difference equations<a class="anchor" aria-label="anchor" href="#solve-difference-equations"></a>
</h2>
<div class="section level3">
<h3 id="arp-systems">AR(p) Systems<a class="anchor" aria-label="anchor" href="#arp-systems"></a>
</h3>
<p><span class="math display">\[
y_t = a_1 y_{t-1} + \cdots + a_p y_{t-p} + u_t =
(a_p,\ldots,a_1)(y_{t-p},\ldots,y_{t-1}) + u_t  
\]</span></p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p><span class="citation">(Scherrer and Deistler 2019)</span>.</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-ScherrerDeistler2019" class="csl-entry">
Scherrer, Wolfgang, and Manfred Deistler. 2019. <span>“Chapter 6 -
Vector Autoregressive Moving Average Models.”</span> In <em>Conceptual
Econometrics Using r</em>, edited by Hrishikesh D. Vinod and C. R. Rao,
41:145–91. Handbook of Statistics. Elsevier. https://doi.org/<a href="https://doi.org/10.1016/bs.host.2019.01.004" class="external-link">https://doi.org/10.1016/bs.host.2019.01.004</a>.
</div>
<div id="ref-Whittle63" class="csl-entry">
Whittle, P. 1963. <span>“<span class="nocase">On the fitting of
multivariate autoregressions, and the approximate canonical
factorization of a spectral density matrix</span>.”</span>
<em>Biometrika</em> 50 (1): 129–34.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://scholar.google.com/citations?user=-Ytb9BYAAAAJ" class="external-link">Wolfgang Scherrer</a>, <a href="https://ch.linkedin.com/in/bernd-funovits-phd-cfa-a8215016" class="external-link">Bernd Funovits</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
