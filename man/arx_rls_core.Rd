% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/05_estimation_rls.R
\name{arx_rls_core}
\alias{arx_rls_core}
\title{RLS function}
\usage{
arx_rls_core(
  y,
  X,
  r = matrix(c(0.9, 0.95, 0.975, 0.9875, 0.99375, 0.996875, 0.9984375, 1), ncol = 1),
  n_init = NULL,
  start_of_eval = NULL,
  end_of_train = NULL,
  enhance_conv = TRUE
)
}
\arguments{
\item{y}{Vector of doubles. Response variable in regression. There must not be NAs in this vector.}

\item{X}{Matrix of doubles. Dimension = (length(y) x maximal number of regressors). NAs are not handled separately (i.e. they must be checked before this function is called).}

\item{r}{Matrix of doubles (a column vector of dimension \eqn{( \text{forgetting\_factors} \times 1)} containing the forgetting factors.}

\item{n_init}{Integer. Number of observations used for initial estimate of beta.
If no value provided, at least 21 observations (3 weeks) or 3 times the number of regressors is used.}

\item{start_of_eval}{Integer. Starting value of evaluation period for honest prediction error
(if we start too early, there are bad initial estimations involved)}

\item{end_of_train}{Integer. Index specifying the end of the training set. (Afterwards the forecasting period starts.)
Important for calculating the honest prediction error (otherwise it would not be out-of-sample...).
comment_bf: If one wants to use arx_rls_core() outside the automated forecasting framework,
set \code{end_of_train} to \code{length(y)}.}

\item{enhance_conv}{Boolean. Indicates whether the convergence enhancing factor as described in Young (2011) page 55.}
}
\value{
List containing
\itemize{
\item\code{y_pred}: vector of predictions (vector of same length as input y)
\item \code{fev_honest}: double. honest prediction error (calculated from \code{end_of_train - start_of_eval + 1} observations)
\item \code{forgetting}: double. forgetting factor which produced the minimal honest prediction error
}
}
\description{
This function implements the Recursive Least Squares (RLS) algorithm with exponentially down-weighted past.
}
