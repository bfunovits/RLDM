% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/05_estimation_likelihood.R
\name{ll_kf}
\alias{ll_kf}
\title{Gaussian log Likelihood of a State Space Model}
\usage{
ll_kf(model, y, method = c("kf", "kf2"), P1 = NULL, a1 = NULL, tol = 0)
}
\arguments{
\item{model}{\code{\link[=stspmod]{stspmod()}} object, which represents the state space model.}

\item{y}{sample, i.e. an \eqn{(N,m)} dimensional matrix,
or a "time series" object (i.e. \code{as.matrix(y)} should return an
\eqn{(N,m)}-dimensional numeric matrix). Missing values (\code{NA}, \code{NaN} and
\code{Inf}) are \strong{not} supported.}

\item{method}{Character string. If \code{method="kf"} then \code{ll_kf} calls
\code{ll_kf_cpp} ("standard form" of the Kalman filter) and for
\code{method="kf2"} the "square root" form of the Kalman filter is used,
i.e. \code{ll_kf2_cpp} is called. Up to numerical errors the outputs should not
depend on the chosen method.}

\item{P1}{\eqn{(s,s)} dimensional covariance matrix of the error of the initial state estimate,
i.e. \eqn{\Pi_{1|0}}{P[1|0]}.
If \code{NULL}, then the state covariance \eqn{P = APA'+B\Sigma B'} is used.
Note that this scheme assumes that the state space model is stable,
i.e. that the state transition matrix \eqn{A} is stable.}

\item{a1}{\eqn{s} dimensional vector, which holds the initial estimate \eqn{a_{1|0}}{a[1|0]}
for the state at time \eqn{t=1}.  If \code{a1=NULL}, then a zero vector is used.}

\item{tol}{(small) tolerance value (or zero). In order to speed up the computations,
the algorithm(s) switch to a constant Kalman gain when there is no significant change in
state error covariance. This behavior is controlled by the parameter \code{tol} and
may be switched off by setting \code{tol=0}.}
}
\value{
(double) The Gaussian log Likelihood of the model.
}
\description{
These routines compute the log Likelihood for time invariant, linear state space models of the form
\deqn{a_{t+1} = A a_t + Bu_t}{a[t+1] = A a[t] + B u[t]}
\deqn{y_t = C a_t + Du_t}{y[t] = C a[t] + D u[t]}
with \eqn{m}-dimensional outputs \eqn{y_t}{y[t]}, \eqn{s}-dimensional states
\eqn{a_t}{a[t]} and \eqn{n}-dimensional disturbances \eqn{u_t}{u[t]}.
The disturbances are white noise with a covariance matrix
\eqn{\mathbf{E} u_t u_t'=\Sigma}{E u[t]u[t]'=\Sigma}.
Note that the disturbances and the outputs may have \emph{different} dimensions, however,
only "wide" systems with (\eqn{m\leq n}{m\le  n}) are implemented.

The Gaussian log likelihood (for the case of Gaussian disturbances
\eqn{u_t\sim N(0,\Sigma)}{u[t] ~ N(0,\Sigma)} and
\eqn{a_1\sim N(a_{1|0},\Pi_{1|0})}{a[1] ~ N(a[1|0],P[1|0])}) here is computed by the
standard Kalman Filter or the square root Kalman filter, see \code{\link[=kf]{kf()}}.
The Kalman filter is a recursive
scheme to compute the linear, least squares predictions
for \eqn{a_{t+1}}{a[t+1]} and \eqn{y_{t+1}}{y[t+1]} given the observations
\eqn{y_t,\ldots,y_1}{y[t],\ldots,y[1]} up to time \eqn{t}. These predictions are notated with
\eqn{a_{t+1|t}}{a[t+1|t]} and \eqn{y_{t+1|t}}{y_[t+1|t]}, the
prediction error for the output \eqn{y_{t+1}}{y[t+1]} is
\eqn{\epsilon_{t+1|t}=(y_{t+1}-y_{t+1|t})}{\epsilon[t+1|t]=(y[t+1]-y[t+1|t])}
and the corresponding variances of the prediction errors are
\deqn{\Pi_{t+1|t}=\mathbf{E}(a_{t+1}-a_{t+1|t})
(a_{t+1}-a_{t+1|t})',}{P[t+1|t]=E(a[t+1]-a_[t+1|t])(a[t+1]-a_[t+1|t])',}
\deqn{\Sigma_{t+1|t}=\mathbf{E}(\epsilon_{t+1|t}
\epsilon_{t+1|t}').}{\Sigma[t+1|t]=E(\epsilon_[t+1|t]\epsilon_[t+1|t]').}

The standard form of the Kalman filter is based on the parameter matrices \eqn{A,C}, the variance of
"state disturbances"
\eqn{Q=\mathbf{E}(Bu_t (Bu_t)')=(B\Sigma B')}{Q=E(Bu[t](Bu[t])')=(B\Sigma B')}, the variance
of the "measurement disturbances"
\eqn{R=\mathbf{E}(Du_t (Du_t)')=(D\Sigma D')}{R=(Du[t](Du[t])')=E(D\Sigma D')} and the covariance
\eqn{S=\mathbf{E}(Bu_t(Du_t)')=(B\Sigma D')}{S=(Bu[t](Du[t])')=E(B\Sigma D')}.
Furthermore we need the initial prediction
\eqn{a_{1|0}}{a[1|0]} and the corresponding error variance
\eqn{\Pi_{1|0}}{P[1|0]}.

For the square root form of the filter we need the "square roots"
\eqn{\Pi_{1|0}^{1/2}}{P[1|0]^{1/2}} and \eqn{\Sigma^{1/2}}, i.e. matrices such that
\eqn{\Pi_{1|0} = \Pi_{1|0}^{1/2} (\Pi_{1|0}^{1/2})'}{P[1|0] = P[1|0]^{1/2} (P[1|0]^{1/2})'}
and \eqn{\Sigma = \Sigma^{1/2}(\Sigma^{1/2})'}. In addition, we define
\eqn{H=(D',B')'\Sigma^{1/2}}.

The (scaled) Gaussian log Likelihood of this model then may be expressed as
\deqn{\frac{-1}{2N}\sum_{t=1}^{N}\left(m\log(2\pi) + \log\det\Sigma_{t|t-1} +
          (y_t - y_{t|t-1})' \Sigma_{t|t-1}^{-1} (y_t - y_{t|t-1}) \right).}{
          (-1/(2N))\sum_{t=1}^{N}[ m log(2\pi)  + log det \Sigma[t|t-1] +
           (y[t] - y[t|t-1])' \Sigma[t|t-1]^{-1} (y[t] - y[t|t-1]) ].}
}
\details{
The core routines are  \code{ll_kf_cpp} and \code{ll_kf2_cpp} which are \pkg{RcppArmadillo} implementations
of the standard and the square root Kalman filter. The function \code{ll_kf} is a wrapper function,
which extracts the necessary parameters from an \code{\link[=stspmod]{stspmod()}} object,
computes the initial covariance matrix \code{P1} and the initial state
estimate \code{a1} (if not provided) and then calls \code{ll_kf_cpp} or \code{ll2_kf_cpp}.

Square root Kalman filter: For the square root
\eqn{\Pi_{1|0}^{1/2}}{P[1|0]^{1/2}} the procedure first tries the Cholesky decomposition.
If this fails (since \eqn{\Pi_{1|0}^{1/2}}{P[1|0]^{1/2}} is (close to) singular),
then \code{ll_kf} tries to compute a symmetric square root via the eigenvalue decomposition
of \eqn{\Pi_{1|0}^{1/2}}{P[1|0]^{1/2}}.
}
\section{Notes}{


The procedures only accept "wide" state space systems (\eqn{m \leq n}{m \le n}), since for
"tall" systems (\eqn{m > n}) the variance of the prediction errors
(\eqn{\Sigma_{t+1|t}}{\Sigma[t+1|t]}) is singular for \eqn{t} larger than some threshold.
}

\examples{
s = 4  # state dimension
m = 2  # number of outputs
n = m  # number of inputs (square case m=n)
n.obs = 100 # sample size

# generate a (stable) state space model (in innovation form)
tmpl = tmpl_stsp_full(m, n, s, sigma_L = "chol")
model = r_model(tmpl, bpoles = 1, sd = 0.5)
# generate a sample
data = sim(model, n.obs = n.obs, a1 = NA)

# compute Q, R, S and P1
sigma_L = model$sigma_L
sigma = tcrossprod(sigma_L)
R = model$sys$D \%*\% sigma \%*\% t(model$sys$D)
S = model$sys$B \%*\% sigma \%*\% t(model$sys$D)
Q = model$sys$B \%*\% sigma \%*\% t(model$sys$B)
P1 = lyapunov(model$sys$A, Q)

# compute H and square root of P1
H = rbind(model$sys$D, model$sys$B) \%*\% sigma_L
P1_R = chol(P1)

# compute logLikelihood (via Kalman Filter)
ll = ll_kf(model, data$y)

# compute logLikelihood (via square root Kalman Filter)
ll_test = ll_kf(model, data$y, method = 'kf2')
all.equal(ll, ll_test)

# Note: ll_kf_cpp and ll_kf2_cpp are internal C++ implementations
# called via .Call() by ll_kf()

# call the "full" kf routines
out = kf(model, data$y)
all.equal(ll, out$ll)
out = kf(model, data$y, method = 'kf2')
all.equal(ll, out$ll)
}
\seealso{
\code{\link[=kf]{kf()}} for computation of the predictions \eqn{a_{t+1|t}}{a[t+1|t]} and
\eqn{y_{t+1|t}}{y[t+1|t]}.
}
