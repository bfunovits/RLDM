% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/aed_predict_methods.R
\name{predict}
\alias{predict}
\alias{predict.armamod}
\alias{predict.stspmod}
\alias{evaluate_prediction}
\title{Model Predictions}
\usage{
\method{predict}{armamod}(object, y, h = 1, n.ahead = 0, ...)

\method{predict}{stspmod}(object, y, x, h = 1, n.ahead = 0, ...)

evaluate_prediction(
  y,
  yhat,
  h,
  criteria = list("RMSE"),
  benchmark = NULL,
  samples = list(1:nrow(y))
)
}
\arguments{
\item{object}{\code{rldm_varma} or \code{rldm_ss} object which represents the model.}

\item{y}{\eqn{(T,n)} matrix with the observed outputs (\eqn{y_t}{y[t]}, \eqn{t=1,...,T}).}

\item{h}{(integer) vector of forecast horizons.}

\item{n.ahead}{(integer) number of time steps to look ahead (out of sample). This number is
also denoted with \eqn{T_0}{T0}.}

\item{...}{not used.}

\item{x}{\eqn{(T+T_0,r)}{(T+T0,r)} matrix with the exogenous inputs
(\eqn{x_t}{x[t]}, \eqn{t=1,...,T+T_0}{t=1,...,T+T0}). This input parameter is ignored,
if the model has no exogenous inputs. Note that the condition forecasts
are computed and hence (for a model with exogenous inputs) we need the
values of the inputs up to time \eqn{t=T+T_0}{t=T+T0}.}

\item{yhat}{\eqn{(T,n,l)} dimensional array of forecasts. The entries \code{yhat[t,,i]}
should contain the prediction for \eqn{y_t}{y[t]}.}

\item{criteria}{is a list with "evaluation criteria". See below for more details.}

\item{benchmark}{\eqn{(T,n,l)} dimensional array with "benchmark" forecasts. If \code{NULL}
then the naive (\eqn{h}-step ahead) forecasts are used as benchmark.}

\item{samples}{is a list with "(sub) samples". See below for more details.}
}
\value{
The function \code{predict} returns a list with components
\item{yhat}{\eqn{(T,n,l)} dimensional array for the h-step ahead forecast. \eqn{T}
is the sample size, \eqn{n} is the dimension of the outputs \eqn{y_t}{y[t]}
and \eqn{l} is the number of forecasts made, i.e. the length of the
vector \code{h}. The entries \code{yhat[t,,i]} are the \code{h[i]}-step
ahead forecast for \eqn{y_t}{y[t]}.}
\item{sigmahat}{\eqn{(n,n,l)} dimensional array, where \code{sigmahat[,,i]} contains the theoretical
covariance matrix of the \eqn{h}-step ahead prediction error for \code{h=h[i]}.}
\item{h}{the (integer) vector of forecasts horizons considered.}
\item{yhat.ahead}{\eqn{(T_0,n)}{(T0,n)} dimensional matrix, which contains the "out-of-sample" forecasts
for \eqn{t=T+1, t=T+2,...,t=T+T_0}{t=T+1, t=T+2, ..., t=T+T0}.}
\item{sigmahat.ahead}{\eqn{(n,n,T_0)}{(n,n,T0)} dimensional array, where \code{sigmahat.ahead[,,h]}
contains the theoretical covariance matrix of the \eqn{h}-step ahead prediction
error.}
\item{y,x}{the original data.}
The function \code{evaluate_prediction}  returns a 4-dimensional array where the dimensions refer to
the evaluation criteria, the (sub) samples, the predictors and the components of the output \eqn{y_t}{y[t]}.
Note that the evaluation criteria are applied to the (\eqn{n}) individual components as well as to
the joint vector and hence the 4-th dimension of the array has size \eqn{n+1}.
E.g. if we consider the "RMSE" and the "MAE" of the forecast errors, two samples (a training sample and a
test sample), 5 forecast horizons \eqn{h=1,\ldots,5}{h=1,...,5} and a process
\eqn{(y_t)}{(y[t])} with 2 components, then the result will be a \eqn{(2,2,5,3)}-dimensional array.
}
\description{
Compute the forecasts based on a VARMA or state space model. The procedure implements a simplified approach.
It uses the formulas for the prediction from an infinite past and sets the unknown initial values
(prior to \eqn{t < 1}) simply to zero. This simple approach assumes that the model is \emph{stable} and
\emph{strictly miniphase} and thus the disturbances \eqn{u_t}{u[t]} are the innovations of the process.
Note also that the forecasts for \emph{known} exogenous inputs
are calculated, i.e. the "conditional forecasts". For an \emph{honest} prediction,
forecasts of the exogenous inputs should be used.
\cr
The forecast error covariance matrix computed assumes that the true model is used.
The error which stems from the estimation of the model is not taken into account.
\cr
\cr
The utility function \code{evaluate_prediction} may be used to assess the quality of predictions.
}
\details{
The utility function \code{evaluate_prediction} may be used to asses the quality of some given predictions.
(E.g. as computed by \code{predict}). The evaluation criteria to be used are passed as parameter \code{criteria}
to the function. This parameter is a list with components which are either character strings (for selection of
one of the implemented quality measures) or a user defined function. Such a function takes three arguments
\code{fun(uhat, y, utilde)} where \code{uhat} is a matrix of prediction errors,
\code{y} is a matrix of the (corresponding) true values and \code{ytilde} is a matrix with
the predictions of a benchmark prediction procedures. This allows to compute relative error measures.

The benchmark predictions are passed on via the parameter \code{benchmark} to the procedure.
If this input parameter is missing then the naive \eqn{h}-step ahead predictions are used a benchmark.
(Therefore the user also has to specify the respective forecast horizons via the paramater \code{h}.)

The following evaluation criteria are implemented (\eqn{\hat{y}_{it}}{uhat[it]} denotes
the prediction error for \eqn{y_{it}}{y[it]} and \eqn{\tilde{u}_{it}}{utilde[it]} is the corresponding
error of the benchmark procedure.)
\describe{
\item{MSE}{Mean Square Error}
\item{RMSE}{Root Mean Square Error}
\item{MAE}{Mean Absolute Error}
\item{MdAE}{Median Absolute Error}
\item{MAPE}{Mean Absolute Percentage Error
\eqn{100 mean(|\hat{u}_{it}/y_{it}|)}{100 mean(|uhat[it]/y[it]|)}}
\item{MdAPE}{Median Absolute Percentage Error
\eqn{100 median(|\hat{u}_{it}/y_{it}|)}{100 median(|uhat[it]/y[it]|)}}
\item{RMdSPE}{Root Median Square Percentage Error
\eqn{100 \sqrt{median(\hat{u}^2_{it}/y^2_{it})}}{100 \sqrt{median(uhat[it]^2 / y[it]^2)}}}
\item{RelRMSE}{Relative Root Mean Square Error
\eqn{\sqrt{mean(\hat{u}^2_{it})}/\sqrt{mean(\tilde{u}^2_{it})}}{\sqrt{mean(uhat[it]^2)}/\sqrt{mean(utilde[it]^2)}}}
\item{RelMAE}{Relative Mean Absolute Error
\eqn{mean(|\hat{u}_{it}|)/mean(|\tilde{u}^2_{it}|)}{mean(|uhat[it]|)/mean(|utilde[it]|)}}
\item{PB}{Percentage better
\eqn{mean(100 I(|\hat{u}_{it}|< |\tilde{u}_{it}|))}{mean(100 I(|uhat[it]| < |utilde[it]|))}}
\item{HR}{Hit Rate
\eqn{100 mean( I((\tilde{u}_{it} - \hat{u}_{it})\tilde{u}_{it} \geq 0))}{100 mean( I((utilde[it] - uhat[it])utilde[it] \ge 0))}.
To be precise this measure computes the hit rate only if the naive prediction is the benchmark.}
}

The procedure also supports the evaluation on different (sub) samples.
The parameter \code{samples} is simply list of integer vectors, where each vector defines a sub sample.
E.g. for daily data, one could evaluate the predictions for different weekdays.
}
\examples{

# create a "random" ARMA(1,1) model (stable and miniphase)
model = test_armamod(dim = c(2,2), degrees = c(1,1), bpoles = 1, bzeroes = 1)

# generate data (sample size n.obs = 200, "burn_in" phase has length 100.)
data = sim(model, n.obs = 200, n.burn_in = 100)

# predict with true model
pred_true = predict(model, data$y, h = c(1,5))

# estimate AR model, order selection by AIC
n.train = 110   # use the first 110 observation for estimation
n.test = 90     # the last 90 observations are used for (a fair) comparison
model_ar = est_ar(data$y[1:n.train,],  mean_estimate = "zero",
                  ic = 'AIC', method = 'ols')$model

# predict with AR model
pred_ar = predict(model_ar, data$y, h = c(1,5))

# estimate AR1 model (Yule-Walker)
model_ar1 = est_ar(data$y[1:n.train,],  mean_estimate = "zero",
                  penalty = -1, p.max = 1, method = 'yule-walker')$model

# predict with AR1 model
pred_ar1 = predict(model_ar1, data$y, h = c(1,5))

# evaluate prediction of the AR model (with the AR1 prediction as benchmark)
stats = evaluate_prediction(data$y, pred_ar$yhat, h = pred_ar$h,
                            criteria = list('RMSE', 'MAE', 'PB'),
                            samples = list(train = 21:n.train, test = (n.train+1):(n.train+n.test)),
                            benchmark = pred_ar1$yhat)

# use array2data.frame for "tabular" display of the results
print(array2data.frame(stats, rows = 1:3, cols = 4))

# evaluate all predictions
# join predictions
yhat  = dbind(3, pred_true$yhat, pred_ar1$yhat, pred_ar$yhat)

# define a function to compute the "Median Relative Absolute Error"
MdRAE_ = function(u.hat, y, u.bench){
   stats::median(abs(u.hat/u.bench), na.rm = TRUE)
}
stats = evaluate_prediction(data$y, yhat,
                            h = c(pred_true$h, pred_ar1$h, pred_ar$h),
                            criteria = list('RMSE', 'MAE', MdRAE = MdRAE_),
                            samples = list(train = 21:n.train, test = (n.train+1):(n.train+n.test)))

# split prediction method and forecast horizon
dimnames.stats = dimnames(stats)
stats = stats[,,c(1,3,5,2,4,6),]
dim(stats) = c(3,2,3,2,3)
dimnames(stats) = list(criterion = dimnames.stats[[1]], sample = dimnames.stats[[2]],
                      model = c('true','AR1','AR'), h = paste('h=',c(1,5),sep=''),
                      data = dimnames.stats[[4]])

# use array2data.frame for "tabular" display of the results
print(array2data.frame(stats, cols = 5, rows = c(3,4,1,2)))
}
